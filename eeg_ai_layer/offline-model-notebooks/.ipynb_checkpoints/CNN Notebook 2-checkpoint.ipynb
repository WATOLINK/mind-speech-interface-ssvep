{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ab6f8d",
   "metadata": {},
   "source": [
    "## New CNN Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfeb5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import numpy.matlib as npm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "from scipy.signal import butter, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df5bb8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.layers import Dense, LSTM, Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Conv2D, BatchNormalization\n",
    "from keras.layers import Input,Flatten, Dense\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Conv2D, BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import initializers, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5469e",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f7d256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass_filter(data, lowcut, highcut, sample_rate, order):\n",
    "    '''\n",
    "    Returns bandpass filtered data between the frequency ranges specified in the input.\n",
    "    Args:\n",
    "        data (numpy.ndarray): array of samples. \n",
    "        lowcut (float): lower cutoff frequency (Hz).\n",
    "        highcut (float): lower cutoff frequency (Hz).\n",
    "        sample_rate (float): sampling rate (Hz).\n",
    "        order (int): order of the bandpass filter.\n",
    "    Returns:\n",
    "        (numpy.ndarray): bandpass filtered data.\n",
    "    '''\n",
    "    \n",
    "    nyq = 0.5 * sample_rate\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98bb18ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer(data, duration, data_overlap):\n",
    "    '''\n",
    "    Returns segmented data based on the provided input window duration and overlap.\n",
    "    Args:\n",
    "        data (numpy.ndarray): array of samples. \n",
    "        duration (int): window length (number of samples).\n",
    "        data_overlap (int): number of samples of overlap.\n",
    "    Returns:\n",
    "        (numpy.ndarray): segmented data of shape (number_of_segments, duration).\n",
    "    '''\n",
    "    \n",
    "    number_segments = int(np.ceil((len(data) - data_overlap)/(duration - data_overlap)))\n",
    "    temp_buf = [data[i:i+duration] for i in range(0, len(data), (duration - int(data_overlap)))]\n",
    "    temp_buf[number_segments-1] = np.pad(temp_buf[number_segments-1],\n",
    "                                         (0, duration-temp_buf[number_segments-1].shape[0]),\n",
    "                                         'constant')\n",
    "    segmented_data = np.vstack(temp_buf[0:number_segments])\n",
    "    \n",
    "    return segmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d268662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_eeg(csvnames, flicker_freq):\n",
    "\n",
    "    # Establishing what flicker frequencies are present\n",
    "    flicker_freq_dict = dict()    \n",
    "    \n",
    "    \n",
    "    for x in range(len(csvnames)):\n",
    "        path = os.path.split(os.getcwd())[0] + '/data/' + csvnames[x] + '.csv'\n",
    "\n",
    "        df = pd.read_csv (path)\n",
    "\n",
    "        #count value for zero cases\n",
    "        count = 0\n",
    "\n",
    "        # Adding row keys of relevant frequencies from dataframe \n",
    "        for i, freq_point in enumerate(df['Frequency']):\n",
    "            if not np.isnan(freq_point): \n",
    "\n",
    "                #zero case\n",
    "                if freq_point == 0 and count < (1 + num_stims):\n",
    "                    count +=1\n",
    "\n",
    "                elif freq_point == 0 and count == (1 + num_stims):\n",
    "\n",
    "                    if freq_point not in flicker_freq_dict.keys():\n",
    "                        flicker_freq_dict.update({freq_point: [i]})\n",
    "                    else:\n",
    "                        flicker_freq_dict[freq_point].append(i)\n",
    "\n",
    "                    count = 1\n",
    "\n",
    "                #normal case  \n",
    "                elif freq_point not in flicker_freq_dict.keys():\n",
    "                    flicker_freq_dict.update({freq_point: [i]})\n",
    "                else:\n",
    "                    flicker_freq_dict[freq_point].append(i)\n",
    "\n",
    "    flicker_freq = np.array(list(flicker_freq_dict.keys()))\n",
    "\n",
    "    # Formatting the eeg data -> making the appropriate matrix\n",
    "    # Initializing the dimensions of the eeg matrix\n",
    "\n",
    "    num_classes = len(flicker_freq) \n",
    "    n_ch = 8 \n",
    "    total_trial_len = 1114 \n",
    "\n",
    "    #scales to number of trials in csv for each freq\n",
    "    num_trials = min(len(flicker_freq_dict[key]) for key in flicker_freq)\n",
    "\n",
    "    #instantiates eeg data in 4 dimensional np array\n",
    "    eeg = np.zeros((num_classes,n_ch,total_trial_len,num_trials))\n",
    "\n",
    "    # Assigning the correct values to the matrix/object\n",
    "\n",
    "    start_idx_list = []\n",
    "\n",
    "\n",
    "    #grabs start and endpoints for each frequency flash\n",
    "    for i, freq in enumerate(flicker_freq):\n",
    "        for j in range(num_trials):\n",
    "            start_idx = flicker_freq_dict[freq][j]\n",
    "            start_idx_list.append(start_idx)\n",
    "            end_idx = start_idx + total_trial_len\n",
    "\n",
    "            #shaves off timestamps and markers and does a transpose, we transpose it back and cast as a np array\n",
    "            eeg[i, :, :, j] = np.array(df.iloc[start_idx:end_idx, 1:9]).transpose((1,0))\n",
    "\n",
    "    return eeg, flicker_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dc3906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no overlapping buffer, were keeping it simple for now (ask aravind later)\n",
    "def get_filtered_eeg(eeg, lowcut, highcut, order, sample_rate):\n",
    "    '''\n",
    "    Returns bandpass filtered eeg for all channels and trials.\n",
    "    Args:\n",
    "        eeg (numpy.ndarray): raw eeg data of shape (num_classes, num_channels, num_samples, num_trials).\n",
    "        lowcut (float): lower cutoff frequency (Hz).\n",
    "        highcut (float): lower cutoff frequency (Hz).\n",
    "        order (int): order of the bandpass filter.\n",
    "        sample_rate (float): sampling rate (Hz).\n",
    "    Returns:\n",
    "        (numpy.ndarray): bandpass filtered eeg of shape (num_classes, num_channels, num_samples, num_trials).\n",
    "    '''\n",
    "    \n",
    "    num_classes = eeg.shape[0]\n",
    "    num_chan = eeg.shape[1]\n",
    "    total_trial_len = eeg.shape[2]\n",
    "    num_trials = eeg.shape[3]\n",
    "\n",
    "    #instantiate object to be sent to BP filter\n",
    "    filtered_data = np.zeros((eeg.shape[0], eeg.shape[1], total_trial_len, eeg.shape[3]))\n",
    "\n",
    "    \n",
    "    for target in range(0, num_classes):\n",
    "        for channel in range(0, num_chan):\n",
    "            for trial in range(0, num_trials):\n",
    "                \n",
    "                #data to be filtered\n",
    "                signal_to_filter = np.squeeze( eeg[target, channel, 0:total_trial_len, trial] )\n",
    "                \n",
    "                #call to BP filter\n",
    "                filtered_data[target, channel, :, trial] = butter_bandpass_filter(signal_to_filter, \n",
    "                                                                                  lowcut, highcut, \n",
    "                                                                                  sample_rate, order)\n",
    "                \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5c9c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmented_epochs(data, window_len, shift_len, sample_rate):\n",
    "    '''\n",
    "    Returns epoched eeg data based on the window duration and step size.\n",
    "    Args:\n",
    "        data (numpy.ndarray): array of samples. \n",
    "        window_len (int): window length (seconds).\n",
    "        shift_len (int): step size (seconds).\n",
    "        sample_rate (float): sampling rate (Hz).\n",
    "    Returns:\n",
    "        (numpy.ndarray): epoched eeg data of shape. \n",
    "        (num_classes, num_channels, num_trials, number_of_segments, duration).\n",
    "    '''\n",
    "    \n",
    "    num_classes = data.shape[0]\n",
    "    num_chan = data.shape[1]\n",
    "    num_trials = data.shape[3]\n",
    "    \n",
    "    duration = int(window_len*sample_rate)\n",
    "    data_overlap = (window_len - shift_len)*sample_rate\n",
    "    \n",
    "    number_of_segments = int(np.ceil((data.shape[2] - data_overlap)/\n",
    "                                       (duration - data_overlap)))\n",
    "    \n",
    "    segmented_data = np.zeros((data.shape[0], data.shape[1], \n",
    "                               data.shape[3], number_of_segments, duration))\n",
    "\n",
    "    for target in range(0, num_classes):\n",
    "        for channel in range(0, num_chan):\n",
    "            for trial in range(0, num_trials):\n",
    "                segmented_data[target, channel, trial, :, :] = buffer(data[target, channel, :, trial], \n",
    "                                                                      duration, data_overlap) \n",
    "    \n",
    "    return segmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93fecf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude_spectrum_features(segmented_data, FFT_PARAMS):\n",
    "    '''\n",
    "    Returns magnitude spectrum features. Fast Fourier Transform computed based on\n",
    "    the FFT parameters provided as input.\n",
    "\n",
    "    Args:\n",
    "        segmented_data (numpy.ndarray): epoched eeg data of shape \n",
    "        (num_classes, num_channels, num_trials, number_of_segments, num_samples).\n",
    "        FFT_PARAMS (dict): dictionary of parameters used for feature extraction.\n",
    "        FFT_PARAMS['resolution'] (float): frequency resolution per bin (Hz).\n",
    "        FFT_PARAMS['start_frequency'] (float): start frequency component to pick from (Hz). \n",
    "        FFT_PARAMS['end_frequency'] (float): end frequency component to pick upto (Hz). \n",
    "        FFT_PARAMS['sampling_rate'] (float): sampling rate (Hz).\n",
    "\n",
    "    Returns:\n",
    "        (numpy.ndarray): magnitude spectrum features of the input EEG.\n",
    "        (n_fc, num_channels, num_classes, num_trials, number_of_segments).\n",
    "    '''\n",
    "    \n",
    "    num_classes = segmented_data.shape[0]\n",
    "    num_chan = segmented_data.shape[1]\n",
    "    num_trials = segmented_data.shape[2]\n",
    "    number_of_segments = segmented_data.shape[3]\n",
    "    fft_len = segmented_data[0, 0, 0, 0, :].shape[0]\n",
    "\n",
    "    NFFT = round(FFT_PARAMS['sampling_rate']/FFT_PARAMS['resolution'])\n",
    "    fft_index_start = int(round(FFT_PARAMS['start_frequency']/FFT_PARAMS['resolution']))\n",
    "    fft_index_end = int(round(FFT_PARAMS['end_frequency']/FFT_PARAMS['resolution']))+1\n",
    "\n",
    "    features_data = np.zeros(((fft_index_end - fft_index_start), \n",
    "                              segmented_data.shape[1], segmented_data.shape[0], \n",
    "                              segmented_data.shape[2], segmented_data.shape[3]))\n",
    "    \n",
    "    for target in range(0, num_classes):\n",
    "        for channel in range(0, num_chan):\n",
    "            for trial in range(0, num_trials):\n",
    "                for segment in range(0, number_of_segments):\n",
    "                    temp_FFT = np.fft.fft(segmented_data[target, channel, trial, segment, :], NFFT)/fft_len\n",
    "                    magnitude_spectrum = 2*np.abs(temp_FFT)\n",
    "                    features_data[:, channel, target, trial, segment] = magnitude_spectrum[fft_index_start:fft_index_end,]\n",
    "    \n",
    "    return features_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7995db59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_spectrum_features(segmented_data, FFT_PARAMS):\n",
    "    '''\n",
    "    Returns complex spectrum features. Fast Fourier Transform computed based on\n",
    "    the FFT parameters provided as input. The real and imaginary parts of the input\n",
    "    signal are concatenated into a single feature vector.\n",
    "\n",
    "    Args:\n",
    "        segmented_data (numpy.ndarray): epoched eeg data of shape \n",
    "        (num_classes, num_channels, num_trials, number_of_segments, num_samples).\n",
    "        FFT_PARAMS (dict): dictionary of parameters used for feature extraction.\n",
    "        FFT_PARAMS['resolution'] (float): frequency resolution per bin (Hz).\n",
    "        FFT_PARAMS['start_frequency'] (float): start frequency component to pick from (Hz). \n",
    "        FFT_PARAMS['end_frequency'] (float): end frequency component to pick upto (Hz). \n",
    "        FFT_PARAMS['sampling_rate'] (float): sampling rate (Hz).\n",
    "\n",
    "    Returns:\n",
    "        (numpy.ndarray): complex spectrum features of the input EEG.\n",
    "        (2*n_fc, num_channels, num_classes, num_trials, number_of_segments)\n",
    "    '''\n",
    "    \n",
    "    num_classes = segmented_data.shape[0]\n",
    "    num_chan = segmented_data.shape[1]\n",
    "    num_trials = segmented_data.shape[2]\n",
    "    number_of_segments = segmented_data.shape[3]\n",
    "    fft_len = segmented_data[0, 0, 0, 0, :].shape[0]\n",
    "\n",
    "    NFFT = round(FFT_PARAMS['sampling_rate']/FFT_PARAMS['resolution'])\n",
    "    fft_index_start = int(round(FFT_PARAMS['start_frequency']/FFT_PARAMS['resolution']))\n",
    "    fft_index_end = int(round(FFT_PARAMS['end_frequency']/FFT_PARAMS['resolution']))+1\n",
    "\n",
    "    features_data = np.zeros((2*(fft_index_end - fft_index_start), \n",
    "                              segmented_data.shape[1], segmented_data.shape[0], \n",
    "                              segmented_data.shape[2], segmented_data.shape[3]))\n",
    "    \n",
    "    for target in range(0, num_classes):\n",
    "        for channel in range(0, num_chan):\n",
    "            for trial in range(0, num_trials):\n",
    "                for segment in range(0, number_of_segments):\n",
    "                    temp_FFT = np.fft.fft(segmented_data[target, channel, trial, segment, :], NFFT)/fft_len\n",
    "                    real_part = np.real(temp_FFT)\n",
    "                    imag_part = np.imag(temp_FFT)\n",
    "                    features_data[:, channel, target, trial, segment] = np.concatenate((\n",
    "                        real_part[fft_index_start:fft_index_end,], \n",
    "                        imag_part[fft_index_start:fft_index_end,]), axis=0)\n",
    "    \n",
    "    return features_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54bbd85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model(input_shape, CNN_PARAMS):\n",
    "    '''\n",
    "    Returns the Concolutional Neural Network model for SSVEP classification.\n",
    "\n",
    "    Args:\n",
    "        input_shape (numpy.ndarray): shape of input training data \n",
    "        e.g. [num_training_examples, num_channels, n_fc] or [num_training_examples, num_channels, 2*n_fc].\n",
    "        CNN_PARAMS (dict): dictionary of parameters used for feature extraction.        \n",
    "        CNN_PARAMS['batch_size'] (int): training mini batch size.\n",
    "        CNN_PARAMS['epochs'] (int): total number of training epochs/iterations.\n",
    "        CNN_PARAMS['droprate'] (float): dropout ratio.\n",
    "        CNN_PARAMS['learning_rate'] (float): model learning rate.\n",
    "        CNN_PARAMS['lr_decay'] (float): learning rate decay ratio.\n",
    "        CNN_PARAMS['l2_lambda'] (float): l2 regularization parameter.\n",
    "        CNN_PARAMS['momentum'] (float): momentum term for stochastic gradient descent optimization.\n",
    "        CNN_PARAMS['kernel_f'] (int): 1D kernel to operate on conv_1 layer for the SSVEP CNN. \n",
    "        CNN_PARAMS['n_ch'] (int): number of eeg channels\n",
    "        CNN_PARAMS['num_classes'] (int): number of SSVEP targets/classes\n",
    "\n",
    "    Returns:\n",
    "        (keras.Sequential): CNN model.\n",
    "    '''\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(2*CNN_PARAMS['n_ch'], kernel_size=(CNN_PARAMS['n_ch'], 1), \n",
    "                     input_shape=(input_shape[0], input_shape[1], input_shape[2]), \n",
    "                     padding=\"valid\", kernel_regularizer=regularizers.l2(CNN_PARAMS['l2_lambda']), \n",
    "                     kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(CNN_PARAMS['droprate']))  \n",
    "    model.add(Conv2D(2*CNN_PARAMS['n_ch'], kernel_size=(1, CNN_PARAMS['kernel_f']), \n",
    "                     kernel_regularizer=regularizers.l2(CNN_PARAMS['l2_lambda']), padding=\"valid\", \n",
    "                     kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(CNN_PARAMS['droprate']))  \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(CNN_PARAMS['num_classes'], activation='softmax', \n",
    "                    kernel_regularizer=regularizers.l2(CNN_PARAMS['l2_lambda']), \n",
    "                    kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d047f74e",
   "metadata": {},
   "source": [
    "## PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ddebbfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define params\n",
    "CNN_PARAMS = {\n",
    "    'batch_size': 64,\n",
    "    'epochs': 250,\n",
    "    'droprate': 0.25,\n",
    "    'learning_rate': 0.001,\n",
    "    'lr_decay': 0.0,\n",
    "    'l2_lambda': 0.0001,\n",
    "    'momentum': 0.9,\n",
    "    'kernel_f': 10,\n",
    "    'n_ch': 8, \n",
    "    'num_classes': 5} # can be changed\n",
    "\n",
    "all_acc = np.zeros((10, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03c11f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up some completely fixed parameters\n",
    "FFT_PARAMS = {\n",
    "    'resolution': 0.2930,\n",
    "    'start_frequency': 0.0,\n",
    "    'end_frequency': 35.0,\n",
    "    'sampling_rate': 250\n",
    "}\n",
    "\n",
    "magnitude_spectrum_features = dict()\n",
    "\n",
    "flicker_freq = []\n",
    "\n",
    "#change this depending on the number of stimuli in the data\n",
    "num_stims = 4\n",
    "\n",
    "#window and shift in time (seconds)\n",
    "\n",
    "#left at 4.456 seconds (total time of one trail for one class) by default for no overlapping segments, only one segment\n",
    "# in mikas notebook these were both set to 1s\n",
    "window_len = 4.456\n",
    "shift_len = 4.456\n",
    "\n",
    "sample_rate = FFT_PARAMS['sampling_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090b5c22",
   "metadata": {},
   "source": [
    "## DATA Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5df6b40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 8, 1114, 15)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#REPLACE CSV NAME WITH CSV OF INTEREST\n",
    "csvnames = ['174_2022_040508', '174_2022_123780', '174_2022_729377']\n",
    "\n",
    "eeg, flicker_freq = ingest_eeg(csvnames, flicker_freq)\n",
    "\n",
    "#combines epoched data across all csvs along trials axis\n",
    "eeg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a21e7",
   "metadata": {},
   "source": [
    "## DATA Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49f57314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 8, 1114, 15)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wrapper function for EEG data filtering with 4th order BP\n",
    "filtered_data = get_filtered_eeg(eeg, 9, 17, 4, sample_rate)\n",
    "filtered_data.shape #(classes, channels, # of samples, # of trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74922cba",
   "metadata": {},
   "source": [
    "# Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4514e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.75 10.25 11.75 14.75  0.  ]\n"
     ]
    }
   ],
   "source": [
    "print(flicker_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2515b990",
   "metadata": {},
   "source": [
    "## DATA Manufacturing via WINDOW length & SHIFT length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a36953e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 8, 15, 1, 1114)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!!!!!!!!!! set to full window size and no shift by default, change params to change this\n",
    "\n",
    "segmented_data = get_segmented_epochs(filtered_data, window_len, shift_len, sample_rate)\n",
    "segmented_data.shape #(classes, channels, trials, number of windowed segments, total number of samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e62edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in all_segmented_data.keys():\n",
    "    magnitude_spectrum_features[subject] = magnitude_spectrum_features(all_segmented_data[subject], \n",
    "                                                                          FFT_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422ab031",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_spectrum_features['s1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a9492a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50c041ec",
   "metadata": {},
   "source": [
    "# DATA Train/Validation Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dd1cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcnn_training_data = dict()\n",
    "mcnn_results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e9fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get training data\n",
    "\n",
    "def get_training_data(features_data):\n",
    "    features_data = np.reshape(features_data, (features_data.shape[0], features_data.shape[1], \n",
    "                                               features_data.shape[2], \n",
    "                                               features_data.shape[3]*features_data.shape[4]))\n",
    "    train_data = features_data[:, :, 0, :].T\n",
    "    for target in range(1, features_data.shape[2]):\n",
    "        train_data = np.vstack([train_data, np.squeeze(features_data[:, :, target, :]).T])\n",
    "\n",
    "    train_data = np.reshape(train_data, (train_data.shape[0], train_data.shape[1], \n",
    "                                         train_data.shape[2], 1))\n",
    "    total_epochs_per_class = features_data.shape[3]\n",
    "    features_data = []\n",
    "    class_labels = np.arange(CNN_PARAMS['num_classes'])\n",
    "    labels = (npm.repmat(class_labels, total_epochs_per_class, 1).T).ravel()\n",
    "    labels = to_categorical(labels)\n",
    "    \n",
    "    return train_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcnn_training_data = dict()\n",
    "mcnn_results = dict()\n",
    "\n",
    "for subject in all_segmented_data.keys():\n",
    "    mcnn_training_data[subject] = dict()\n",
    "   \n",
    "    train_data, labels = get_training_data(magnitude_spectrum_features[subject])\n",
    "    mcnn_training_data[subject]['train_data'] = train_data\n",
    "    mcnn_training_data[subject]['label'] = labels\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcnn_training_data['s1']['train_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b34d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcnn_training_data['s1']['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_train_test(data, labels):\n",
    "   \n",
    "   \"\"\"Splits data into training and validation set\"\"\"\n",
    "   n_data, n_labels = shuffle(data,labels, random_state = 0)\n",
    "\n",
    "   split_index = int(np.round(len(n_data)*0.8) )\n",
    "   print(split_index)\n",
    "\n",
    "   train_data = n_data[:split_index]\n",
    "   test_data = n_data[split_index:]\n",
    "\n",
    "   train_labels = n_labels[:split_index]\n",
    "   test_labels = n_labels[split_index:]\n",
    "\n",
    "   return (train_data, test_data, train_labels, test_labels) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427d5165",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = split_train_test(mcnn_training_data['s1']['train_data'], mcnn_training_data['s1']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a15e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4de9a0",
   "metadata": {},
   "source": [
    "# Training CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae9a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./CNN_files/model.h5') #*make sure h5 model is named \"model.h5\" and in working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19684368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build new model\n",
    "new_model = Sequential()\n",
    "\n",
    "for layer in model.layers[:-1]: # go through until last layer\n",
    "    new_model.add(layer)\n",
    "#new_model.add(Dense(13, activation='softmax'))\n",
    "\n",
    "\n",
    "new_model.add(Dense(5, activation='softmax'))\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf80c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in new_model.layers[:-1]:\n",
    "  layer.trainable = False \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db96e30",
   "metadata": {},
   "source": [
    "## Compile The New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a36188",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb83b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcnn_training_data['s1']['train_data'][8].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = new_model.fit(train_data, train_labels, epochs = 100, validation_data = (test_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e59ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d985ffd7",
   "metadata": {},
   "source": [
    "# Model Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1913cd1a",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1772ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = new_model.predict(test_data)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a5217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preds = []\n",
    "for i in range(0, len(preds)):\n",
    "  new_preds.append(np.argmax(preds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1263a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e5be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = np.argmax(test_labels, axis = -1)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369668e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_results(g, new_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e71b094",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.save('4_freq_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff7b59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_model = tf.keras.models.load_model('4_freq_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e59fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ddba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mcnn_training_data['s1']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(0, len(mcnn_training_data['s1']['label']), 8):\n",
    "  labels.append(np.argmax(mcnn_training_data['s1']['label'][i]))\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd880de",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_results(labels, newer_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a557dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce331dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preds "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03245db",
   "metadata": {},
   "source": [
    "## Evaluating performance and getting metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2006c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_spectrum_features['s1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa55813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False): \n",
    "  \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n",
    "  If classes is passed, confusion matrix will be labelled, if not, integer class values\n",
    "  will be used.\n",
    "  Args:\n",
    "    y_true: Array of truth labels (must be same shape as y_pred).\n",
    "    y_pred: Array of predicted labels (must be same shape as y_true).\n",
    "    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n",
    "    figsize: Size of output figure (default=(10, 10)).\n",
    "    text_size: Size of output figure text (default=15).\n",
    "    norm: normalize values or not (default=False).\n",
    "    savefig: save confusion matrix to file (default=False).\n",
    "  \n",
    "  Returns:\n",
    "    A labelled confusion matrix plot comparing y_true and y_pred.\n",
    "  Example usage:\n",
    "    make_confusion_matrix(y_true=test_labels, # ground truth test labels\n",
    "                          y_pred=y_preds, # predicted labels\n",
    "                          classes=class_names, # array of class label names\n",
    "                          figsize=(15, 15),\n",
    "                          text_size=10)\n",
    "  \"\"\"  \n",
    "  # Create the confustion matrix\n",
    "  cm = confusion_matrix(y_true, y_pred)\n",
    "  cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n",
    "  n_classes = cm.shape[0] # find the number of classes we're dealing with\n",
    "\n",
    "  # Plot the figure and make it pretty\n",
    "  fig, ax = plt.subplots(figsize=figsize)\n",
    "  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n",
    "  fig.colorbar(cax)\n",
    "\n",
    "  # Are there a list of classes?\n",
    "  if classes:\n",
    "    labels = classes\n",
    "  else:\n",
    "    labels = np.arange(cm.shape[0])\n",
    "  \n",
    "  # Label the axes\n",
    "  ax.set(title=\"Confusion Matrix\",\n",
    "         xlabel=\"Predicted label\",\n",
    "         ylabel=\"True label\",\n",
    "         xticks=np.arange(n_classes), # create enough axis slots for each class\n",
    "         yticks=np.arange(n_classes), \n",
    "         xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n",
    "         yticklabels=labels)\n",
    "  \n",
    "  # Make x-axis labels appear on bottom\n",
    "  ax.xaxis.set_label_position(\"bottom\")\n",
    "  ax.xaxis.tick_bottom()\n",
    "\n",
    "  # Set the threshold for different colors\n",
    "  threshold = (cm.max() + cm.min()) / 2.\n",
    "\n",
    "  # Plot the text on each cell\n",
    "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    if norm:\n",
    "      plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
    "              horizontalalignment=\"center\",\n",
    "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
    "              size=text_size)\n",
    "    else:\n",
    "      plt.text(j, i, f\"{cm[i, j]}\",\n",
    "              horizontalalignment=\"center\",\n",
    "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
    "              size=text_size)\n",
    "\n",
    "  # Save the figure to the current working directory\n",
    "  if savefig:\n",
    "    fig.savefig(\"confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6271ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_results(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
    "  Args:\n",
    "      y_true: true labels in the form of a 1D array\n",
    "      y_pred: predicted labels in the form of a 1D array\n",
    "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
    "  \"\"\"\n",
    "  # Calculate model accuracy\n",
    "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "  # Calculate model precision, recall and f1 score using \"weighted average\n",
    "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "  model_results = {\"accuracy\": model_accuracy,\n",
    "                  \"precision\": model_precision,\n",
    "                  \"recall\": model_recall,\n",
    "                  \"f1\": model_f1}\n",
    "  return model_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
